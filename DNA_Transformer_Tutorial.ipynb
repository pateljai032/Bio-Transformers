{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNA Transformer - Complete Implementation\n",
    "## K-mers, BPE, One-hot Encoding, and Transformer Models\n",
    "\n",
    "This notebook demonstrates the improved implementation with:\n",
    "- Robust FASTA reading\n",
    "- Three encoding methods (K-mers, BPE, One-hot)\n",
    "- Transformer architecture\n",
    "- Complete training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation (uncomment if needed)\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install biopython\n",
    "# !pip install tokenizers\n",
    "# !pip install numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dna_transformer_improved import (\n",
    "    FASTAReader,\n",
    "    KmerTokenizer,\n",
    "    DNABPETokenizer,\n",
    "    OneHotEncoder,\n",
    "    DNASequenceDataset,\n",
    "    DNATransformerEncoder,\n",
    "    DNATransformerOneHot,\n",
    "    collate_fn_tokens,\n",
    "    collate_fn_onehot,\n",
    "    train_epoch,\n",
    "    evaluate\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading FASTA Files\n",
    "\n",
    "The improved `FASTAReader` handles both gzipped and regular FASTA files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Read from your actual FASTA file\n",
    "# Uncomment and replace with your file path\n",
    "# fasta_reader = FASTAReader(\"your_file.fasta.gz\")\n",
    "# sequences_data = fasta_reader.read_sequences(limit=100)\n",
    "# \n",
    "# # Display sequence information\n",
    "# for i, seq in enumerate(sequences_data[:3]):\n",
    "#     print(f\"\\nSequence {i+1}:\")\n",
    "#     print(f\"  ID: {seq['id']}\")\n",
    "#     print(f\"  V Number: {seq['v_number']}\")\n",
    "#     print(f\"  Sample: {seq['sample_number']}\")\n",
    "#     print(f\"  Length: {seq['length']:,} bases\")\n",
    "#     print(f\"  First 50 bases: {seq['sequence'][:50]}\")\n",
    "# \n",
    "# # Get statistics\n",
    "# stats = fasta_reader.get_sequence_stats()\n",
    "# print(f\"\\nDataset Statistics:\")\n",
    "# for key, value in stats.items():\n",
    "#     print(f\"  {key}: {value:,.0f}\")\n",
    "# \n",
    "# sequences = [seq['sequence'] for seq in sequences_data]\n",
    "\n",
    "# Option 2: Use example sequences\n",
    "sequences = [\n",
    "    \"ATGCTAGCTAGCTAGCTAGCTAATGCTAGCGATCGATCGTAGCTAGCTGATCGATCGATCGATCGTAGCTAGCTAGCTAGCTA\",\n",
    "    \"GGCTACGTTACGACGTAACGTACGATCGATCGATCGTAGCTAGCTACGATCGATCGACGATCGATCGTAGCTAGCTAGCTAGC\",\n",
    "    \"TTACTGACCTGAACCTGACCTACGATCGATCGATCGTAGCTAGCTAGCTAGCTAGCTACGATCGATCGTAGCTAGCTAGCTAGC\",\n",
    "    \"ACGTACGTACGTACGTACGTACGATCGATCGATCGTAGCTAGCTAGCTAGCTAGCTACGATCGATCGTAGCTAGCTAGCTAGCT\",\n",
    "    \"ATGCGGATCCGATCGATCGATCGATCGATCGATCGTAGCTAGCTAGCTAGCTAGCTACGATCGATCGTAGCTAGCTAGCTAGCT\",\n",
    "    \"GGCATGCTAGCATCGATGCATGCGATCGATCGATCGTAGCTAGCTAGCTAGCTAGCTACGATCGATCGTAGCTAGCTAGCTAGC\",\n",
    "    \"TTACGATCGATCGTGCACGATCGATCGATCGTAGCTAGCTAGCTAGCTAGCTAGCTACGATCGATCGTAGCTAGCTAGCTAGCT\",\n",
    "    \"CGATCGATCGATCGTAGCTAGCTACGATCGATCGATCGATCGATCGATCGTAGCTAGCTAGCTAGCTAGCTACGATCGATCGTA\",\n",
    "] * 10  # Replicate for more data\n",
    "\n",
    "print(f\"Loaded {len(sequences)} sequences\")\n",
    "print(f\"Average length: {np.mean([len(s) for s in sequences]):.0f} bases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-mer Tokenization\n",
    "\n",
    "K-mers split sequences into overlapping or non-overlapping subsequences of length k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create k-mer tokenizer\n",
    "k = 6\n",
    "stride = 3  # 1 for overlapping, k for non-overlapping\n",
    "\n",
    "kmer_tokenizer = KmerTokenizer(k=k, stride=stride)\n",
    "kmer_tokenizer.build_vocab(sequences)\n",
    "\n",
    "print(f\"K-mer settings: k={k}, stride={stride}\")\n",
    "print(f\"Vocabulary size: {len(kmer_tokenizer.vocab)}\")\n",
    "print(f\"\\nSpecial tokens: {kmer_tokenizer.special_tokens}\")\n",
    "print(f\"\\nSample k-mers from vocabulary:\")\n",
    "sample_kmers = list(kmer_tokenizer.vocab.keys())[8:18]\n",
    "for kmer in sample_kmers:\n",
    "    print(f\"  {kmer}: {kmer_tokenizer.vocab[kmer]}\")\n",
    "\n",
    "# Test encoding/decoding\n",
    "test_seq = sequences[0][:60]\n",
    "encoded = kmer_tokenizer.encode(test_seq)\n",
    "decoded = kmer_tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"\\nTest Sequence: {test_seq}\")\n",
    "print(f\"Encoded IDs:   {encoded}\")\n",
    "print(f\"Decoded:       {decoded}\")\n",
    "print(f\"Match: {test_seq[:len(decoded)] == decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-mer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count k-mer frequencies\n",
    "from collections import Counter\n",
    "\n",
    "all_kmers = []\n",
    "for seq in sequences:\n",
    "    all_kmers.extend(kmer_tokenizer.sequence_to_kmers(seq))\n",
    "\n",
    "kmer_counts = Counter(all_kmers)\n",
    "top_kmers = kmer_counts.most_common(10)\n",
    "\n",
    "print(\"Top 10 most frequent k-mers:\")\n",
    "for kmer, count in top_kmers:\n",
    "    print(f\"  {kmer}: {count:,} occurrences\")\n",
    "\n",
    "# Visualize k-mer distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "counts = [count for _, count in top_kmers]\n",
    "labels = [kmer for kmer, _ in top_kmers]\n",
    "plt.bar(range(len(counts)), counts)\n",
    "plt.xlabel('K-mer')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10 K-mer Frequencies')\n",
    "plt.xticks(range(len(labels)), labels, rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "freq_distribution = list(kmer_counts.values())\n",
    "plt.hist(freq_distribution, bins=50, edgecolor='black')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Number of K-mers')\n",
    "plt.title('K-mer Frequency Distribution')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BPE Tokenization\n",
    "\n",
    "BPE learns optimal subword units from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train BPE tokenizer\n",
    "bpe_tokenizer = DNABPETokenizer(vocab_size=1000, min_frequency=2)\n",
    "\n",
    "print(\"Training BPE tokenizer...\")\n",
    "bpe_tokenizer.train(sequences)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Test encoding/decoding\n",
    "test_seq = sequences[0][:60]\n",
    "encoded_bpe = bpe_tokenizer.encode(test_seq)\n",
    "decoded_bpe = bpe_tokenizer.decode(encoded_bpe)\n",
    "\n",
    "print(f\"\\nTest Sequence: {test_seq}\")\n",
    "print(f\"Encoded IDs:   {encoded_bpe}\")\n",
    "print(f\"Decoded:       {decoded_bpe}\")\n",
    "print(f\"Match: {test_seq == decoded_bpe}\")\n",
    "\n",
    "# Compare compression\n",
    "print(f\"\\nCompression comparison:\")\n",
    "print(f\"  Original length: {len(test_seq)} bases\")\n",
    "print(f\"  K-mer tokens:    {len(encoded)} tokens\")\n",
    "print(f\"  BPE tokens:      {len(encoded_bpe)} tokens\")\n",
    "print(f\"  BPE compression: {len(test_seq) / len(encoded_bpe):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. One-Hot Encoding\n",
    "\n",
    "One-hot encoding represents each base as a 4-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encoder\n",
    "onehot_encoder = OneHotEncoder(include_ambiguous=True)\n",
    "\n",
    "# Test encoding\n",
    "test_seq = \"ATGCN\"\n",
    "encoded_onehot = onehot_encoder.encode(test_seq)\n",
    "\n",
    "print(\"One-hot encoding example:\")\n",
    "print(f\"Sequence: {test_seq}\")\n",
    "print(f\"\\nEncoded shape: {encoded_onehot.shape}\")\n",
    "print(f\"\\nEncoding (A, C, G, T):\")\n",
    "for i, base in enumerate(test_seq):\n",
    "    print(f\"  {base}: {encoded_onehot[i]}\")\n",
    "\n",
    "# Visualize one-hot encoding\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(encoded_onehot.T, cmap='Blues', aspect='auto')\n",
    "plt.colorbar(label='Value')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Base (A, C, G, T)')\n",
    "plt.title('One-Hot Encoding Visualization')\n",
    "plt.yticks([0, 1, 2, 3], ['A', 'C', 'G', 'T'])\n",
    "plt.xticks(range(len(test_seq)), list(test_seq))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test decoding\n",
    "decoded_onehot = onehot_encoder.decode(encoded_onehot)\n",
    "print(f\"\\nDecoded: {decoded_onehot}\")\n",
    "print(f\"Match: {test_seq == decoded_onehot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Dataset and DataLoader\n",
    "\n",
    "Choose your encoding method: 'kmer', 'bpe', or 'onehot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "ENCODING_TYPE = 'kmer'  # Change to 'bpe' or 'onehot' to try different methods\n",
    "MAX_SEQ_LENGTH = 128\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Choose tokenizer based on encoding type\n",
    "if ENCODING_TYPE == 'kmer':\n",
    "    tokenizer = kmer_tokenizer\n",
    "elif ENCODING_TYPE == 'bpe':\n",
    "    tokenizer = bpe_tokenizer\n",
    "else:  # onehot\n",
    "    tokenizer = None\n",
    "\n",
    "# Split data\n",
    "split_idx = int(0.8 * len(sequences))\n",
    "train_sequences = sequences[:split_idx]\n",
    "val_sequences = sequences[split_idx:]\n",
    "\n",
    "print(f\"Encoding method: {ENCODING_TYPE}\")\n",
    "print(f\"Training sequences: {len(train_sequences)}\")\n",
    "print(f\"Validation sequences: {len(val_sequences)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DNASequenceDataset(\n",
    "    train_sequences,\n",
    "    tokenizer,\n",
    "    max_length=MAX_SEQ_LENGTH,\n",
    "    encoding_type=ENCODING_TYPE\n",
    ")\n",
    "\n",
    "val_dataset = DNASequenceDataset(\n",
    "    val_sequences,\n",
    "    tokenizer,\n",
    "    max_length=MAX_SEQ_LENGTH,\n",
    "    encoding_type=ENCODING_TYPE\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "collate_fn = collate_fn_onehot if ENCODING_TYPE == 'onehot' else collate_fn_tokens\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "D_MODEL = 128\n",
    "NHEAD = 4\n",
    "NUM_LAYERS = 2\n",
    "DIM_FEEDFORWARD = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Create model based on encoding type\n",
    "if ENCODING_TYPE == 'onehot':\n",
    "    model = DNATransformerOneHot(\n",
    "        input_dim=4,\n",
    "        d_model=D_MODEL,\n",
    "        nhead=NHEAD,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dim_feedforward=DIM_FEEDFORWARD,\n",
    "        dropout=DROPOUT,\n",
    "        num_classes=4\n",
    "    )\n",
    "    vocab_size = 4\n",
    "else:\n",
    "    vocab_size = len(tokenizer.vocab)\n",
    "    model = DNATransformerEncoder(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=D_MODEL,\n",
    "        nhead=NHEAD,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dim_feedforward=DIM_FEEDFORWARD,\n",
    "        dropout=DROPOUT,\n",
    "        max_seq_length=MAX_SEQ_LENGTH\n",
    "    )\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model: {model.__class__.__name__}\")\n",
    "print(f\"Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(f\"Training for {NUM_EPOCHS} epochs...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(\n",
    "        model, train_loader, optimizer, criterion, device, ENCODING_TYPE\n",
    "    )\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = evaluate(\n",
    "        model, val_loader, criterion, device, ENCODING_TYPE\n",
    "    )\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss:   {val_loss:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "        }, f'best_model_{ENCODING_TYPE}.pt')\n",
    "        print(\"âœ“ Saved best model\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"Training complete! Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss', marker='o', linewidth=2)\n",
    "plt.plot(val_losses, label='Val Loss', marker='s', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training Curves - {ENCODING_TYPE.upper()} Encoding')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "overfitting = np.array(val_losses) - np.array(train_losses)\n",
    "plt.plot(epochs, overfitting, marker='o', linewidth=2, color='red')\n",
    "plt.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation - Training Loss')\n",
    "plt.title('Overfitting Metric')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'training_curves_{ENCODING_TYPE}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot saved: training_curves_{ENCODING_TYPE}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(f'best_model_{ENCODING_TYPE}.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Test on a few sequences\n",
    "test_sequences = val_sequences[:3]\n",
    "\n",
    "print(\"Testing model predictions:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, test_seq in enumerate(test_sequences):\n",
    "    test_seq = test_seq[:100]  # Use first 100 bases\n",
    "    \n",
    "    print(f\"\\nTest Sequence {i+1}:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    if ENCODING_TYPE == 'onehot':\n",
    "        # One-hot encoding\n",
    "        encoder = OneHotEncoder()\n",
    "        test_input = encoder.encode_tensor(test_seq).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(test_input)\n",
    "        \n",
    "        predicted = torch.argmax(output, dim=-1)[0]\n",
    "        bases = ['A', 'C', 'G', 'T']\n",
    "        predicted_seq = ''.join([bases[i] for i in predicted.cpu().numpy()])\n",
    "    else:\n",
    "        # Token-based encoding\n",
    "        test_input = torch.tensor(tokenizer.encode(test_seq)).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(test_input)\n",
    "        \n",
    "        predicted = torch.argmax(output, dim=-1)[0]\n",
    "        predicted_seq = tokenizer.decode(predicted.cpu().tolist())\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    min_len = min(len(test_seq), len(predicted_seq))\n",
    "    matches = sum(1 for j in range(min_len) if test_seq[j] == predicted_seq[j])\n",
    "    accuracy = matches / min_len\n",
    "    \n",
    "    print(f\"Original:  {test_seq[:50]}...\")\n",
    "    print(f\"Predicted: {predicted_seq[:50]}...\")\n",
    "    print(f\"Accuracy:  {accuracy:.2%} ({matches}/{min_len} bases correct)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Models and Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer/encoder\n",
    "if ENCODING_TYPE == 'kmer':\n",
    "    kmer_tokenizer.save_vocab(f'kmer_vocab_k{k}.json')\n",
    "    print(f\"Saved: kmer_vocab_k{k}.json\")\n",
    "elif ENCODING_TYPE == 'bpe':\n",
    "    bpe_tokenizer.save(f'bpe_tokenizer.json')\n",
    "    print(f\"Saved: bpe_tokenizer.json\")\n",
    "\n",
    "print(f\"Model checkpoint: best_model_{ENCODING_TYPE}.pt\")\n",
    "print(\"\\nAll files saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Comparison\n",
    "\n",
    "### Encoding Method Comparison\n",
    "\n",
    "| Method | Pros | Cons | Best For |\n",
    "|--------|------|------|----------|\n",
    "| **K-mer** | Fast, interpretable, good for motifs | Fixed vocabulary, may miss long-range patterns | Genomic sequences, motif discovery |\n",
    "| **BPE** | Learns optimal units, flexible vocabulary | Requires training, less interpretable | Large diverse datasets, compression |\n",
    "| **One-hot** | Simple, preserves base information | High memory, no compression | Short sequences, base-level predictions |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Experiment with different k values** (3, 6, 9) for k-mer encoding\n",
    "2. **Try different stride values** to balance sequence length and information\n",
    "3. **Increase model size** (d_model, num_layers) for better performance\n",
    "4. **Train on larger datasets** for improved generalization\n",
    "5. **Fine-tune for specific tasks** (e.g., promoter prediction, variant calling)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "python3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
